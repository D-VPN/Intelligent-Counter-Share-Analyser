{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "israeli-mixer",
   "metadata": {},
   "source": [
    "### Using Tensorflow for object detection and extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-possible",
   "metadata": {},
   "source": [
    "#### We will implementing transfer learning with ImageAI for custom object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educated-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-brisbane",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-windows",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "connected-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import xml.etree.ElementTree as xm\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, Resizing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Lambda, Input, Reshape\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from time import strftime\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-sociology",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-luxembourg",
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf_images = \"./GroceryDataset/images/ShelfImages/\"\n",
    "product_images = \"./GroceryDataset/images/ProductImagesFromShelves/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-overhead",
   "metadata": {},
   "source": [
    "# Trying out the raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forbidden-employer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\"))\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-straight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crucial-communications",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\resnet50_coco_best_v2.1.0.h5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lyric-algebra",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\rect1.png'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(execution_path , \"rect1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unknown-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle  :  61.22475266456604\n",
      "vase  :  56.356436014175415\n"
     ]
    }
   ],
   "source": [
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"Production Assets/Shelf.jpg\"), output_image_path=os.path.join(execution_path , \"imagenew.jpg\"))\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "revolutionary-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# open_tarfile=tarfile.open(\"GroceryDataset_part1.tar.gz\")\n",
    "# open_tarfile.extractall(path='GroceryDataset_part1')\n",
    "# open_tarfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "right-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_tarfile=tarfile.open(\"GroceryDataset_part2.tar.gz\")\n",
    "# open_tarfile.extractall(path='GroceryDataset_part2')\n",
    "# open_tarfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-hampshire",
   "metadata": {},
   "source": [
    "# Formating the data set in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "copyrighted-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_files = [ f for f in os.listdir(f'{shelf_images}') if f.endswith('JPG') ]\n",
    "photos_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in jpg_files], columns=['file', 'shelf_id', 'planogram_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "['C1_P01_N1_S2_1.JPG', 'C1_P01_N1_S2_2.JPG', 'C1_P01_N1_S3_1.JPG', 'C1_P01_N1_S3_2.JPG', 'C1_P01_N1_S5_1.JPG']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P01_N1_S2_2.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1_P01_N1_S3_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1_P01_N1_S3_2.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1_P01_N1_S5_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S5_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id\n",
       "0  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1\n",
       "1  C1_P01_N1_S2_2.JPG   C1_P01      N1_S2_2\n",
       "2  C1_P01_N1_S3_1.JPG   C1_P01      N1_S3_1\n",
       "3  C1_P01_N1_S3_2.JPG   C1_P01      N1_S3_2\n",
       "4  C1_P01_N1_S5_1.JPG   C1_P01      N1_S5_1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(jpg_files))\n",
    "print(jpg_files[:5])\n",
    "photos_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enclosed-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.DataFrame([[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))]\n",
    "                           for i in range(11)\n",
    "                           for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n",
    "                          columns = ['file', 'shelf_id', 'planogram_id', 'category', 'xmin', 'ymin', 'w', 'h'])\n",
    "\n",
    "# convert from width, height to xmax, ymax\n",
    "\n",
    "products_df['xmax'] = products_df['xmin'] + products_df['w']\n",
    "products_df['ymax'] = products_df['ymin'] + products_df['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incoming-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13184, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "      <th>category</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1008</td>\n",
       "      <td>1552</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>1260</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1028</td>\n",
       "      <td>928</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>1280</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>872</td>\n",
       "      <td>244</td>\n",
       "      <td>392</td>\n",
       "      <td>268</td>\n",
       "      <td>1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>1568</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>532</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>872</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>544</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id  category  xmin  ymin    w    h  \\\n",
       "0  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0  1008  1552  252  376   \n",
       "1  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0  1028   928  252  376   \n",
       "2  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0    24   872  244  392   \n",
       "3  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0   280  1568  252  376   \n",
       "4  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0   292   872  252  376   \n",
       "\n",
       "   xmax  ymax  \n",
       "0  1260  1928  \n",
       "1  1280  1304  \n",
       "2   268  1264  \n",
       "3   532  1944  \n",
       "4   544  1248  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(products_df.shape)\n",
    "(products_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-doctor",
   "metadata": {},
   "source": [
    "# converting in pascal VOC format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-chester",
   "metadata": {},
   "source": [
    "#### Done in Create Annotation file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-status",
   "metadata": {},
   "source": [
    "# Train with yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prostate-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "subjective-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beginning-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.74\n",
      "Anchor Boxes generated.\n",
      "Detection configuration saved in  C:\\Users\\Nikhil\\Desktop\\Hackathon\\Intelligent-Conuter-Share-Analyzer\\ML Models\\GroceryDataset\\production\\json\\detection_config.json\n",
      "Evaluating over 50 samples taken from C:\\Users\\Nikhil\\Desktop\\Hackathon\\Intelligent-Conuter-Share-Analyzer\\ML Models\\GroceryDataset\\production\\validation\n",
      "Training over 100 samples  given at C:\\Users\\Nikhil\\Desktop\\Hackathon\\Intelligent-Conuter-Share-Analyzer\\ML Models\\GroceryDataset\\production\\train\n",
      "Training on: \t['0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "Training with Batch Size:  2\n",
      "Number of Training Samples:  100\n",
      "Number of Validation Samples:  50\n",
      "Number of Experiments:  10\n",
      "Training with transfer learning from pretrained Model\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\anaconda3\\envs\\tf_2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer YoloLayer has arguments in `__init__` and therefore must override `get_config`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\anaconda3\\envs\\tf_2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/400 [..............................] - ETA: 1:49:26 - loss: 133.1132 - yolo_layer_loss: 16.3151 - yolo_layer_1_loss: 27.8740 - yolo_layer_2_loss: 77.3437"
     ]
    }
   ],
   "source": [
    "execution_path = os.getcwd()\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=os.path.join(execution_path ,\"GroceryDataset\\production\"))\n",
    "trainer.setTrainConfig(object_names_array=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], batch_size=2, num_experiments=10, train_from_pretrained_model=\"pretrained-yolov3.h5\")\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-challenge",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sixth-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\anaconda3\\envs\\tf_2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  72.59123921394348  :  [1174, 1688, 1795, 2217]\n",
      "0  :  64.21286463737488  :  [1102, 1370, 1601, 2626]\n",
      "0  :  63.40987682342529  :  [1284, 1527, 1953, 2469]\n"
     ]
    }
   ],
   "source": [
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"trained models/detection_model-ex-001--loss-0099.677.h5\") \n",
    "detector.setJsonPath(\"trained models/detection_config.json\")\n",
    "detector.loadModel()\n",
    "detections = detector.detectObjectsFromImage(input_image=\"GroceryDataset/images/ShelfImages/C1_P01_N1_S3_1.JPG\", output_image_path=\"detected-shelf-image.JPG\")\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "female-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nikhil\\anaconda3\\envs\\tf_2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  58.8617742061615  :  [1569, 174, 2115, 1001]\n",
      "0  :  50.22878050804138  :  [874, 165, 1569, 1105]\n",
      "0  :  67.92833209037781  :  [1268, 211, 1879, 1063]\n",
      "0  :  61.567145586013794  :  [515, 240, 1210, 1118]\n",
      "0  :  65.38127064704895  :  [1779, 193, 2346, 1164]\n",
      "0  :  86.40586733818054  :  [653, 1008, 1164, 2428]\n",
      "2  :  62.969303131103516  :  [653, 1008, 1164, 2428]\n",
      "4  :  52.91258096694946  :  [653, 1008, 1164, 2428]\n",
      "4  :  57.59460926055908  :  [909, 1085, 1440, 2355]\n",
      "4  :  54.27485704421997  :  [1145, 1074, 1647, 2366]\n",
      "0  :  88.93685340881348  :  [1195, 1093, 1686, 2347]\n",
      "4  :  56.82775378227234  :  [1400, 1098, 2014, 2344]\n",
      "0  :  89.18904662132263  :  [860, 1039, 1400, 2482]\n",
      "2  :  67.0357882976532  :  [860, 1039, 1400, 2482]\n",
      "2  :  66.38590097427368  :  [1124, 1009, 1669, 2512]\n",
      "0  :  88.03472518920898  :  [1375, 1042, 1953, 2479]\n",
      "2  :  63.676267862319946  :  [1375, 1042, 1953, 2479]\n",
      "0  :  77.75135636329651  :  [1719, 1130, 2315, 2401]\n",
      "2  :  53.96694540977478  :  [1719, 1130, 2315, 2401]\n",
      "0  :  60.20021438598633  :  [0, 1534, 1301, 2656]\n",
      "0  :  54.14295196533203  :  [476, 1654, 1080, 2537]\n",
      "0  :  84.02248620986938  :  [657, 1762, 1513, 2511]\n",
      "4  :  51.41936540603638  :  [647, 1757, 1611, 2518]\n",
      "2  :  72.19074964523315  :  [798, 1793, 1635, 2482]\n",
      "0  :  89.41273093223572  :  [1068, 1772, 1814, 2505]\n",
      "2  :  53.41007709503174  :  [1256, 1793, 1892, 2485]\n",
      "0  :  86.18178367614746  :  [1578, 1829, 2279, 2448]\n",
      "2  :  64.99107480049133  :  [1578, 1829, 2279, 2448]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype <U42 cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0b33f4a4fd48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mextracted_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   2722\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m         data=None, **kwargs):\n\u001b[1;32m-> 2724\u001b[1;33m     __ret = gca().imshow(\n\u001b[0m\u001b[0;32m   2725\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    698\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[0;32m    699\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[1;32m--> 700\u001b[1;33m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0m\u001b[0;32m    701\u001b[0m                             \"float\".format(self._A.dtype))\n\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Image data of dtype <U42 cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from imageai.Detection.Custom import CustomObjectDetection\n",
    "\n",
    "detector = CustomObjectDetection()\n",
    "detector.setModelTypeAsYOLOv3()\n",
    "detector.setModelPath(\"trained models/detection_model-ex-002--loss-0064.244.h5\") \n",
    "detector.setJsonPath(\"trained models/detection_config.json\")\n",
    "detector.loadModel()\n",
    "detections, extracted_images = detector.detectObjectsFromImage(input_image=\"GroceryDataset/images/ShelfImages/C1_P01_N1_S3_1.JPG\", output_image_path=\"detected-shelf-image_2.JPG\",extract_detected_objects=True)\n",
    "for detection in detections:\n",
    "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n",
    "\n",
    "# for img in extracted_images:\n",
    "#     plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-reviewer",
   "metadata": {},
   "source": [
    "# Calculating % visibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "coral-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"detected-shelf-image_2-objects/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coated-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_objects = len(os.listdir(dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smart-healing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-upper",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
