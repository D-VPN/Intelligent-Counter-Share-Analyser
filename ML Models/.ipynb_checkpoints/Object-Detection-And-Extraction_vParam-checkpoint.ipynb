{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Tensorflow for object detection and extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will implementing transfer learning with ImageAI for custom object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import xml.etree.ElementTree as xm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, Resizing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Lambda, Input, Reshape\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from time import strftime\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-50beea708774>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgpu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection import ObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf_images = \"./GroceryDataset/images/ShelfImages/\"\n",
    "product_images = \"./GroceryDataset/images/ProductImagesFromShelves/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out the raw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "execution_path = os.getcwd()\n",
    "\n",
    "detector = ObjectDetection()\n",
    "detector.setModelTypeAsRetinaNet()\n",
    "detector.setModelPath( os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\"))\n",
    "detector.loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\resnet50_coco_best_v2.1.0.h5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(execution_path , \"resnet50_coco_best_v2.1.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\rect1.png'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(execution_path , \"rect1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottle  :  61.22475266456604\n",
      "vase  :  56.356436014175415\n"
     ]
    }
   ],
   "source": [
    "detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , \"Production Assets/Shelf.jpg\"), output_image_path=os.path.join(execution_path , \"imagenew.jpg\"))\n",
    "\n",
    "for eachObject in detections:\n",
    "    print(eachObject[\"name\"] , \" : \" , eachObject[\"percentage_probability\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "\n",
    "# open_tarfile=tarfile.open(\"GroceryDataset_part1.tar.gz\")\n",
    "# open_tarfile.extractall(path='GroceryDataset_part1')\n",
    "# open_tarfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_tarfile=tarfile.open(\"GroceryDataset_part2.tar.gz\")\n",
    "# open_tarfile.extractall(path='GroceryDataset_part2')\n",
    "# open_tarfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formating the data set in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpg_files = [ f for f in os.listdir(f'{shelf_images}') if f.endswith('JPG') ]\n",
    "photos_df = pd.DataFrame([[f, f[:6], f[7:14]] for f in jpg_files], columns=['file', 'shelf_id', 'planogram_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "['C1_P01_N1_S2_1.JPG', 'C1_P01_N1_S2_2.JPG', 'C1_P01_N1_S3_1.JPG', 'C1_P01_N1_S3_2.JPG', 'C1_P01_N1_S5_1.JPG']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P01_N1_S2_2.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1_P01_N1_S3_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1_P01_N1_S3_2.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1_P01_N1_S5_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S5_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id\n",
       "0  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1\n",
       "1  C1_P01_N1_S2_2.JPG   C1_P01      N1_S2_2\n",
       "2  C1_P01_N1_S3_1.JPG   C1_P01      N1_S3_1\n",
       "3  C1_P01_N1_S3_2.JPG   C1_P01      N1_S3_2\n",
       "4  C1_P01_N1_S5_1.JPG   C1_P01      N1_S5_1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(jpg_files))\n",
    "print(jpg_files[:5])\n",
    "photos_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_df = pd.DataFrame([[f[:18], f[:6], f[7:14], i, *map(int, f[19:-4].split('_'))]\n",
    "                           for i in range(11)\n",
    "                           for f in os.listdir(f'{product_images}{i}') if f.endswith('png')],\n",
    "                          columns = ['file', 'shelf_id', 'planogram_id', 'category', 'xmin', 'ymin', 'w', 'h'])\n",
    "\n",
    "# convert from width, height to xmax, ymax\n",
    "\n",
    "products_df['xmax'] = products_df['xmin'] + products_df['w']\n",
    "products_df['ymax'] = products_df['ymin'] + products_df['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13184, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>shelf_id</th>\n",
       "      <th>planogram_id</th>\n",
       "      <th>category</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1008</td>\n",
       "      <td>1552</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>1260</td>\n",
       "      <td>1928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1028</td>\n",
       "      <td>928</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>1280</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>872</td>\n",
       "      <td>244</td>\n",
       "      <td>392</td>\n",
       "      <td>268</td>\n",
       "      <td>1264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>1568</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>532</td>\n",
       "      <td>1944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C1_P01_N1_S2_1.JPG</td>\n",
       "      <td>C1_P01</td>\n",
       "      <td>N1_S2_1</td>\n",
       "      <td>0</td>\n",
       "      <td>292</td>\n",
       "      <td>872</td>\n",
       "      <td>252</td>\n",
       "      <td>376</td>\n",
       "      <td>544</td>\n",
       "      <td>1248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file shelf_id planogram_id  category  xmin  ymin    w    h  \\\n",
       "0  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0  1008  1552  252  376   \n",
       "1  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0  1028   928  252  376   \n",
       "2  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0    24   872  244  392   \n",
       "3  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0   280  1568  252  376   \n",
       "4  C1_P01_N1_S2_1.JPG   C1_P01      N1_S2_1         0   292   872  252  376   \n",
       "\n",
       "   xmax  ymax  \n",
       "0  1260  1928  \n",
       "1  1280  1304  \n",
       "2   268  1264  \n",
       "3   532  1944  \n",
       "4   544  1248  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(products_df.shape)\n",
    "(products_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting in pascal VOC format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done in Create Annotation file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with yolov3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageai.Detection.Custom import DetectionModelTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor boxes for training images and annotation...\n",
      "Average IOU for 9 anchors: 0.71\n",
      "Anchor Boxes generated.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\GroceryDataset\\\\production\\\\json\\\\detection_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1a1f30dffe22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetModelTypeAsYOLOv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetDataDirectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_directory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecution_path\u001b[0m \u001b[1;33m,\u001b[0m\u001b[1;34m\"GroceryDataset\\production\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetTrainConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_names_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"4\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"5\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"6\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"7\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"8\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"9\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"10\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_experiments\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_from_pretrained_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pretrained-yolov3.h5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf_2\\lib\\site-packages\\imageai\\Detection\\Custom\\__init__.py\u001b[0m in \u001b[0;36msetTrainConfig\u001b[1;34m(self, object_names_array, batch_size, num_experiments, train_from_pretrained_model)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mjson_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"anchors\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__inference_anchors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__json_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"detection_config.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w+\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m             json.dump(json_data, json_file, indent=4, separators=(\",\", \" : \"),\n\u001b[0;32m    194\u001b[0m                       ensure_ascii=True)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nikhil\\\\Desktop\\\\Hackathon\\\\Intelligent-Conuter-Share-Analyzer\\\\ML Models\\\\GroceryDataset\\\\production\\\\json\\\\detection_config.json'"
     ]
    }
   ],
   "source": [
    "execution_path = os.getcwd()\n",
    "\n",
    "trainer = DetectionModelTrainer()\n",
    "trainer.setModelTypeAsYOLOv3()\n",
    "trainer.setDataDirectory(data_directory=os.path.join(execution_path ,\"GroceryDataset\\production\"))\n",
    "trainer.setTrainConfig(object_names_array=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], batch_size=4, num_experiments=100, train_from_pretrained_model=\"pretrained-yolov3.h5\")\n",
    "trainer.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
